âœ… Your Alerting Criteria Summary
Trigger alert only if:

Pod fails due to:

OOMKilled

CrashLoopBackOff

Container creation failure

Status mismatch: desired â‰  current

Restart threshold: Only alert if pod restarts > 3 times in 10 minutes

Ignore one-off or expected restarts.

Apply only to selected namespaces.

ğŸ” 1. PromQL Query (Sysdig Prometheus-compatible)
Hereâ€™s a PromQL that combines all your requirements:

promql
Copy
Edit
(
  (
    kube_pod_container_status_terminated_reason{reason="OOMKilled", namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}
  or
    kube_pod_container_status_waiting_reason{reason=~"CrashLoopBackOff|ImagePullBackOff", namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}
  or
    kube_pod_status_phase{phase=~"Failed|Unknown", namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}
  or
    (kube_deployment_status_replicas_unavailable{namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"} > 0)
  )
)
or
(
  increase(kube_pod_container_status_restarts_total{namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[10m]) > 3
)
ğŸ“Œ Explanation of Each Metric:
Metric	Purpose
kube_pod_container_status_terminated_reason	Detects OOMKilled
kube_pod_container_status_waiting_reason	CrashLoopBackOff, ImagePullBackOff
kube_pod_status_phase	General pod failures
kube_deployment_status_replicas_unavailable	Desired state â‰  actual
increase(...restarts_total[10m]) > 3	Pod restarting more than 3 times in 10 minutes

âš™ï¸ 2. Configure in Sysdig
When adding this in Sysdig Monitor:

Use this PromQL in a new alert rule.

Scope: Choose â€œAcross All Clustersâ€ or configure for both clusters separately if needed.

Notification Channels: Select both Email and Teams (or webhooks).

Alert Condition: â€œWhenever this expression is > 0â€

Evaluation Window: 5m or 1m (depending on sensitivity)

Severity: Set appropriately (e.g., Critical or Warning)

ğŸ§© 3. Team Introduction Plan
To roll this out smoothly:

a. Acknowledge Previous Noise
â€œWe got some false alerts earlier due to one-off pod restarts. Based on your feedback, Iâ€™ve tuned the logic to suppress unnecessary alerts.â€

b. Present New Logic Briefly
â€œThis new alert will only trigger when something real breaks â€” like OOM, CrashLoopBackOff, actual pod failure, or restarts exceeding 3 times in 10 minutes â€” and only for key namespaces (ns1 to ns10).â€

c. Mention Benefits
Prevent alert fatigue

Reduce noise

Focus only on meaningful failures

Custom to our namespaces

d. Demo + Feedback Loop
Share a dashboard with the new rule in a team meeting (or Slack thread).

Run a test alert with simulated OOMKilled pod.

Ask the team to observe for a few days and provide input.

â€œLetâ€™s monitor this alert for a few days and tweak if needed. If you see something we missed or still feels noisy, we can refine the logic together.â€

ğŸ§ª Optional: Add Alert Labels (for future enhancements)
If Sysdig lets you attach metadata, consider tagging alerts like:

yaml
Copy
Edit
labels:
  severity: "critical"
  alert_group: "pod_failures"
  cluster: "prod"
Would you like me to generate the Sysdig YAML configuration for this alert rule?









Ask ChatGPT
