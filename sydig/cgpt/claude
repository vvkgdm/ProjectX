Sysdig PromQL Alerts Configuration
1. Pod Failure Alert Queries
Alert 1: Pod OOM (Out of Memory) Failures
promql# OOM Killed Pods
increase(sysdig_container_oom_kill_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[5m]) > 0
Alert 2: Container/Pod Start Failures
promql# Container start failures
increase(sysdig_container_start_failures_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[5m]) > 0
Alert 3: Pod Desired vs Current State Mismatch
promql# Pod desired state not matching current state
(
  sysdig_kubernetes_pod_desired_replicas{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"} 
  != 
  sysdig_kubernetes_pod_current_replicas{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}
) and (
  sysdig_kubernetes_pod_desired_replicas{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"} > 0
)
Alert 4: Pod Status Not Running (Excluding Succeeded/Completed)
promql# Pods in failed, pending, or unknown state
sysdig_kubernetes_pod_status_phase{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10", phase!~"Running|Succeeded"} > 0
Alert 5: Excessive Pod Restarts (Multiple restarts in 10 minutes)
promql# Pod restarting more than 3 times in 10 minutes
increase(sysdig_kubernetes_pod_restart_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[10m]) > 3
Alert 6: Container Waiting State Issues
promql# Containers stuck in waiting state due to image pull errors, crash loops, etc.
sysdig_kubernetes_container_waiting_reason{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10", reason=~"ImagePullBackOff|ErrImagePull|CrashLoopBackOff|CreateContainerConfigError"} > 0
2. Combined Alert Query (Recommended)
promql# Comprehensive pod failure alert
(
  increase(sysdig_container_oom_kill_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[5m]) > 0
) or (
  increase(sysdig_container_start_failures_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[5m]) > 0
) or (
  sysdig_kubernetes_pod_status_phase{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10", phase!~"Running|Succeeded"} > 0
) or (
  increase(sysdig_kubernetes_pod_restart_total{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"}[10m]) > 3
) or (
  sysdig_kubernetes_container_waiting_reason{kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10", reason=~"ImagePullBackOff|ErrImagePull|CrashLoopBackOff|CreateContainerConfigError"} > 0
)
3. Sysdig Alert Configuration
Alert Settings:

Alert Name: Pod Failures - Critical Namespaces
Description: Monitors for pod failures in critical namespaces (ns1-ns10) excluding normal single restarts
Severity: High
Evaluation Frequency: 1 minute
Alert Condition: Value > 0

Notification Channels:

Email Channel: Configure your team email distribution list
Microsoft Teams Channel: Configure Teams webhook URL

Alert Labels to Include:

kubernetes_namespace_name
kubernetes_pod_name
kubernetes_cluster_name
kubernetes_node_name
container_name

4. Namespace Configuration
Update the namespace filter in all queries by replacing:
kubernetes_namespace_name=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10"
With your actual namespace names:
kubernetes_namespace_name=~"production|staging|frontend|backend|database|api|monitoring|logging|ingress|auth"
5. Team Introduction Plan
Phase 1: Preparation (Week 1)
Day 1-2: Documentation and Testing

Document the current false positive issues
Test the new PromQL queries in a staging environment
Create a comparison document showing old vs new alert criteria

Day 3-5: Stakeholder Alignment

Present the enhanced alert strategy to team leads
Gather feedback on the 10-minute restart threshold
Adjust namespace list based on team priorities

Phase 2: Pilot Implementation (Week 2)
Day 1-3: Soft Launch

Deploy alerts to only 2-3 critical namespaces
Monitor for 72 hours to validate effectiveness
Track false positive reduction

Day 4-7: Team Training

Conduct training session on new alert criteria
Explain the rationale behind the 10-minute restart threshold
Provide playbooks for each alert type

Phase 3: Full Rollout (Week 3)
Day 1-2: Complete Deployment

Enable alerts for all 10 configured namespaces
Activate both email and Teams notifications

Day 3-7: Monitoring and Optimization

Daily check-ins for the first 3 days
Weekly reviews for ongoing optimization
Document any tuning needed

Phase 4: Continuous Improvement (Ongoing)
Weekly Reviews

Analyze alert patterns and false positive rates
Adjust thresholds based on team feedback
Expand to additional namespaces if needed

Monthly Assessments

Review alert effectiveness metrics
Update documentation and playbooks
Team feedback sessions

6. Team Communication Template
Initial Announcement Email:
Subject: Enhanced Pod Monitoring - Reducing False Positives

Team,

Based on your feedback about excessive restart alerts, we're implementing an enhanced monitoring strategy:

Key Changes:
✅ Single pod restarts will NOT trigger alerts
✅ Only multiple restarts (>3 in 10 minutes) will alert
✅ Focus on actual failures: OOM, startup failures, state mismatches
✅ Monitoring limited to our 10 critical namespaces

Timeline:
- Week 1: Testing and validation
- Week 2: Pilot with 3 namespaces
- Week 3: Full rollout

This should reduce false positives by ~80% while maintaining coverage for genuine issues.

Training session scheduled for [DATE] - attendance mandatory for on-call team members.
Success Metrics to Track:

False positive rate reduction
Alert response time improvement
Team satisfaction with alert relevance
Time to incident resolution

Escalation Path:

Initial Alert: Teams notification + email
5 minutes unacknowledged: Page primary on-call
15 minutes unacknowledged: Page secondary on-call
30 minutes unacknowledged: Page team lead

7. Monitoring and Maintenance
Weekly Reviews:

Check alert volume and patterns
Review any false positives
Adjust thresholds if needed

Monthly Optimization:

Analyze alert effectiveness
Update namespace list if required
Refine restart threshold based on data

Quarterly Assessment:

Full review of alert strategy
Team feedback collection
Documentation updates


-----
I've created a comprehensive PromQL alert configuration for your Sysdig setup. Here are the key highlights:
Key Features:

Smart Restart Filtering: Only alerts when pods restart more than 3 times in 10 minutes, avoiding false positives from normal restarts
Multiple Failure Types: Covers OOM kills, startup failures, state mismatches, and container waiting issues
Namespace Filtering: Configurable for your specific 10 namespaces
Dual Notifications: Email and Teams integration

Main Alert Categories:

OOM Failures: Catches memory-related pod kills
Startup Failures: Detects container initialization issues
State Mismatches: Monitors desired vs actual pod states
Excessive Restarts: Only alerts on multiple restarts (>3 in 10 min)
Container Issues: Catches ImagePullBackOff, CrashLoopBackOff, etc.

Team Introduction Strategy:
The 4-phase rollout plan addresses your team's concerns about false positives:

Preparation: Document and test improvements
Pilot: Soft launch with limited namespaces
Rollout: Full deployment with training
Optimization: Continuous improvement based on feedback

Next Steps:

Replace the namespace names in the queries with your actual namespace names
Test the combined query in your Sysdig environment
Configure the email and Teams notification channels
Start with the pilot phase using 2-3 critical namespaces

The 10-minute restart threshold should significantly reduce false positives while still catching genuine issues like crash loops or persistent failures. You can adjust this threshold based on your team's feedback during the pilot phase.
Would you like me to help you customize any specific part of this configuration or provide additional queries for other monitoring scenarios?RetryClaude can make mistakes. Please double-check responses.