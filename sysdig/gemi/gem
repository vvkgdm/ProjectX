(
    # Triggers if any pod in the specified namespaces/clusters is in a 'Failed' phase.
    sum by (pod, namespace, cluster) (
        kube_pod_status_phase{
            phase="Failed",
            namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10",
            cluster=~"cluster1|cluster2"
        }
    )
    OR
    # Triggers if a deployment has fewer available replicas than desired.
    sum by (deployment, namespace, cluster) (
        kube_deployment_spec_replicas{
            namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10",
            cluster=~"cluster1|cluster2"
        }
        -
        kube_deployment_status_replicas_available{
            namespace=~"ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10",
            cluster=~"cluster1|cluster2"
        }
    )
) > 0

Sysdig Alert Configuration Notes for this Query:

Threshold: > 0 (meaning at least one pod is failed or one replica is missing).

Duration (for clause): Set a for duration (e.g., 5m or 10m) in your Sysdig alert rule. This will prevent alerts for very brief, transient failures and ensure the issue is persistent before notifying.

Severity: Choose an appropriate severity (e.g., Critical or High).


Here are the PromQL queries for Sysdig to address your requirements, along with a plan to introduce these new alerts to your team.

I've designed two separate PromQL queries to cover different aspects of pod failures, ensuring that transient restarts don't trigger unnecessary alerts.

PromQL Queries for Sysdig
These queries assume your Sysdig environment collects standard Kubernetes metrics (like kube_state_metrics). Remember to replace cluster1|cluster2 with the actual regex or list of your cluster names, and ns1|ns2|ns3|ns4|ns5|ns6|ns7|ns8|ns9|ns10 with your exact namespaces.

1. Alert for Persistent Pod Failures and Deployment Issues
This query identifies pods that are in a Failed state or deployments where the number of available replicas is less than the desired number. This covers OOMs (which often lead to failed pods), container startup failures, and general state mismatches.
PromQL for Persistent Pod Failures
8 Jul, 8:59 am

2. Alert for Excessive Pod Restarts
This query specifically addresses your team's feedback regarding single pod restarts not being alert-worthy. It will only trigger if a container within a pod restarts more than 2 times (i.e., 3 or more times) within a 10-minute window.
PromQL for Excessive Pod Restarts
8 Jul, 8:59 am

Open

Plan to Introduce to Your Team
Hereâ€™s a structured approach to present these new alerts to your team, addressing their previous concerns and building confidence:

Subject: Enhancing Kubernetes Pod Failure Alerts - Addressing False Positives

1. Acknowledge and Validate Feedback (Start Positive)

"Team, thanks for your valuable feedback on the previous pod alerts. You rightly pointed out that single or infrequent pod restarts in our production clusters were generating too many false positives and causing alert fatigue."

"We've taken that feedback to heart and have refined our monitoring strategy to be more intelligent and actionable."

2. Explain the Problem with Previous Alerts

"The old alerts were perhaps too sensitive, triggering on every pod restart, even if it was a transient event or part of a normal self-healing process."

"This led to noise and made it harder to identify truly critical issues."

3. Introduce the New Approach & Logic (Focus on Improvement)

"We're introducing a new set of alerts designed to be more precise. The key changes are:"

"Focus on Persistent Failures: We now have an alert that specifically looks for pods that are consistently in a 'Failed' state or deployments that aren't meeting their desired replica counts for a sustained period. This will catch issues like OOMs or persistent container startup failures."

"Intelligent Restart Detection: Crucially, for pod restarts, we've implemented a logic that only alerts if a pod restarts multiple times within a 10-minute window (specifically, 3 or more times). This means a single, isolated restart won't trigger an alert, significantly reducing noise."

"This approach ensures we're notified about genuine, ongoing problems that require intervention, rather than transient events."

4. Clarify Scope

"These new alerts will be configured for our two main production clusters and will specifically monitor the critical namespaces you identified: ns1, ns2, ns3, ..., ns10."

5. Notification Channels

"Notifications for these alerts will continue to be sent to our existing channels: Mail and Teams."

6. Call for Review and Feedback (Foster Collaboration)

"I've drafted the PromQL queries for these new alerts. I'd like to walk you through them and get your input on the thresholds, especially the 'multiple restarts' definition. Does '3 or more restarts in 10 minutes' sound like the right threshold for 'multiple times'?"

"Your insights are crucial to ensure these alerts are effective and useful for everyone."

7. Proposed Testing Phase (Build Confidence)

"My suggestion is that we implement these new alerts and run them in a 'silent' or 'observation' mode for a week or two, if Sysdig allows, or simply monitor their behavior closely. This will allow us to validate their effectiveness and make any necessary adjustments before fully relying on them."

"Once we're confident, we can decommission the older, noisy alerts."

8. Documentation

"I'll also ensure these new alert definitions and their rationale are properly documented in our runbooks/monitoring documentation."

By following this plan, you'll not only introduce effective new monitoring but also demonstrate that you've listened to your team's concerns, fostering a more collaborative and efficient operational environment.










Deep Research


Gemini can make mistakes, so double-check it

PromQL for Persistent Pod Failures